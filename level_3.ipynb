{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 3: Introduction to Modeling\n",
    "\n",
    "(Find this notebook hosted [here](http://nbviewer.jupyter.org/github/kl2806/devfest-data-science-track/blob/master/level_3.ipynb).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're at one of the coolest parts of data science: modeling. When you're analyzing data, you usually have something you want to predict based on other information you have. There are many different ways to \"do prediction,\" and we'll be covering two popular methods: *k*-nearest neighbor (KNN) and linear regression. But before we start, let's cover some terminology and concepts that are good to keep in mind when modeling.\n",
    "\n",
    "We usually call the variable that we're trying to predict the *output variable*; the other features we use to predict said output are called *input variables.* The procedure by which predict a new data point using other data points is our *model*, and as we mentioned before, there are a variety of modeling techniques to choose from. Luckily we don't have to code them all from scratch because Python is awesome: there's a package called `scikit-learn` that does this for us. (As an aside, prediction is called *classification* if the output variable is discrete and *regression* if it is continuous.)\n",
    "\n",
    "So what goes into building a model? Ideally we'd like it to make smart predictions -- it can *learn* from the data we have to detect underlying patterns and relationships. Maybe we could feed it our entire dataset, but then we wouldn't know how well our model actually works since we've just used up all our data. Could we just \"re-use\" some of the data to test out this model's predictions? If we did that, our model would make some pretty spot-on predictions. Coincidence? Not quite. If you think about it, we'd probably all do a lot better in school if we could see copies of exams before we actually take them. But life is hard and cheating is bad, so we shouldn't encourage our models to cheat either.  \n",
    "\n",
    "This means we need to partition out the data: \"hide away\" a *test set* for future validation, and teach our model using a *training set.* There's no magic formula for divvying them up but usually a 60:40 or 80:20 train/test split tends to work well. In this way, our model will have just enough information to make smart predictions on the new test set that it hasn't seen before. \n",
    "\n",
    "To make all this more concrete, let's frame it in terms of our problem. What are we trying to do? We want to predict the precipitation on a given day based on weather data about average temperature, wind speed, etc. collected over roughly 1000 days.\n",
    "\n",
    "Here's our first method: the *k-nearest neighbor*. The name is informative: the *k*-nearest neighbor algorithm says that for any new point, we should average the output values associated with the *k* closest points to it and call that our predicted value. This picture will give us a better idea as to what that means: \n",
    "\n",
    "![Nearest Neighbor Picture](nn.gif)\n",
    "\n",
    "In this picture, *k* = 3 and we want to predict what that green dot could be. The algorithm first looks at the closest 3 points to the green dot and makes an assumption. It says, \"Hey, I'm guessing that points closer to each other will be similar to each other, too.\" This isn't an outrageous assumption to make, especially since we tend to surround ourselves with people we \"click\" with -- people with whom we share certain qualities or characteristics. So this clever algorithm figures out that there are 2B's versus 1A in the dot's *3 closest neighbors* and predicts that the green dot will also be B. Cool, huh?\n",
    "\n",
    "Now, let's use this to actually predict some precipitation! We'll first read in our cleaned up data set from the previous level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month                 int64\n",
      "Day                   int64\n",
      "Year                  int64\n",
      "Mean Temperature      int64\n",
      "Max Temperature       int64\n",
      "Min Temperature       int64\n",
      "Dew Point             int64\n",
      "Average Humidity      int64\n",
      "Maximum Humidity      int64\n",
      "Minimum Humidity      int64\n",
      "Precipitation       float64\n",
      "Wind Speed          float64\n",
      "Max Wind Speed      float64\n",
      "Max Gust Speed      float64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Year</th>\n",
       "      <th>Mean Temperature</th>\n",
       "      <th>Max Temperature</th>\n",
       "      <th>Min Temperature</th>\n",
       "      <th>Dew Point</th>\n",
       "      <th>Average Humidity</th>\n",
       "      <th>Maximum Humidity</th>\n",
       "      <th>Minimum Humidity</th>\n",
       "      <th>Precipitation</th>\n",
       "      <th>Wind Speed</th>\n",
       "      <th>Max Wind Speed</th>\n",
       "      <th>Max Gust Speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>33</td>\n",
       "      <td>40</td>\n",
       "      <td>26</td>\n",
       "      <td>22</td>\n",
       "      <td>54</td>\n",
       "      <td>64</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2013</td>\n",
       "      <td>28</td>\n",
       "      <td>33</td>\n",
       "      <td>22</td>\n",
       "      <td>11</td>\n",
       "      <td>48</td>\n",
       "      <td>57</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2013</td>\n",
       "      <td>28</td>\n",
       "      <td>32</td>\n",
       "      <td>24</td>\n",
       "      <td>14</td>\n",
       "      <td>56</td>\n",
       "      <td>68</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>34</td>\n",
       "      <td>37</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>56</td>\n",
       "      <td>63</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2013</td>\n",
       "      <td>37</td>\n",
       "      <td>42</td>\n",
       "      <td>32</td>\n",
       "      <td>19</td>\n",
       "      <td>48</td>\n",
       "      <td>56</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Month  Day  Year  Mean Temperature  Max Temperature  Min Temperature  \\\n",
       "0      1    1  2013                33               40               26   \n",
       "1      1    2  2013                28               33               22   \n",
       "2      1    3  2013                28               32               24   \n",
       "3      1    4  2013                34               37               30   \n",
       "4      1    5  2013                37               42               32   \n",
       "\n",
       "   Dew Point  Average Humidity  Maximum Humidity  Minimum Humidity  \\\n",
       "0         22                54                64                44   \n",
       "1         11                48                57                39   \n",
       "2         14                56                68                43   \n",
       "3         19                56                63                48   \n",
       "4         19                48                56                39   \n",
       "\n",
       "   Precipitation  Wind Speed  Max Wind Speed  Max Gust Speed  \n",
       "0              0           7              15              26  \n",
       "1              0           6              15              22  \n",
       "2              0           5              13              20  \n",
       "3              0           8              18              28  \n",
       "4              0           7              17              26  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('clean_weather_data.csv')\n",
    "print(data.dtypes)\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like everything's good with the data -- let's now split it up into training and test sets! Thankfully `scikit-learn` already provides a function to do this. We'll use 80% of the data as training data. It's typical to separate your prediction variable from the rest of the data when modeling, so we'll follow this convention as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X = data.drop('Precipitation', axis=1)\n",
    "y = data.Precipitation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now, let's fit our first model. We'll use *k* = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "          metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "          weights='uniform')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn_model = KNeighborsRegressor(n_neighbors=3)\n",
    "knn_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay, fitting a model using `sklearn` is super simple! So how do we know how well our model did? Well, we can define a function (typically called a *loss function* among data nerds) that can take our prediction and true value, and quantify how \"correct\" we were.\n",
    "\n",
    "For example, let's say we predicted $\\hat{y}$ for a case that had a true value of *y*; one common loss function for regression is the squared error loss, or $(y - \\hat{y})^2$. We can then compare $\\sum_{i = 1}^{n}{(y_i - \\hat{y_i})^2}$ to $\\sum_{i = 1}^{n}{(y_i - \\bar{y})^2}$ where $n$ is the number of data points we have. All this really means is adding up the loss for each individual data point. The second term is equivalent to predicting the average for every case, and if the first term is significantly less, we did much better than just random guessing. Hopefully we did well...let's find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 29.9691113861\n",
      "KNN loss: 27.4045222222\n",
      "Variation explained: 8.55744146322%\n"
     ]
    }
   ],
   "source": [
    "knn_y = knn_model.predict(X_test)\n",
    "\n",
    "knn_sum_squares = ((knn_y - y_test) ** 2).sum()\n",
    "mean_sum_squares = ((y_test.mean() - y_test) ** 2).sum()\n",
    "\n",
    "print(\"Average loss:\", mean_sum_squares)\n",
    "print(\"KNN loss:\", knn_sum_squares)\n",
    "print(\"Variation explained: \", 100 * (1 - knn_sum_squares / mean_sum_squares), \"%\", sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call the \"variation explained\" the coefficient of variation, or $r^2$. This quantity tells us how much variation, or interesting information in your data, your model explains in a meaningful way using the relationship between its input and output variables. You can also use the `score` function to automatically calculate this $r^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2:  0.0855744146322\n"
     ]
    }
   ],
   "source": [
    "print(\"R^2: \", knn_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, we managed to fit and analyze our first model! Let's move on to linear regression.\n",
    "\n",
    "Let's first think about predicting an output variable with only one input variable. The basic idea of linear regression is that you can aim to fit a straight line through the scatterplot, like so:\n",
    "\n",
    "![Linear Regression](linear-regression.png)\n",
    "\n",
    "It turns out that you can generalize this to predicting one variable using multiple variables. Let's see how we can put this into code using `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, KNN wasn't a special case, `sklearn` just makes things that easy. Let's see how we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.210857982665\n"
     ]
    }
   ],
   "source": [
    "print(linear_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, linear regression did better than the KNN algorithm. One of the key ideas in data science is that you have to explore which models are best to use with your data. For fun, let's see how the two models compare by plotting the true value and the predicted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sns.plt.scatter(y_test, knn_model.predict(X_test))\n",
    "sns.plt.title('KNN Prediction Analysis')\n",
    "sns.plt.xlabel('Real Value')\n",
    "sns.plt.ylabel('Prediction')\n",
    "plt.show()\n",
    "\n",
    "sns.plt.scatter(y_test, linear_model.predict(X_test))\n",
    "sns.plt.title('Linear Regression Prediction Analysis')\n",
    "sns.plt.xlabel('Real Value')\n",
    "sns.plt.ylabel('Prediction')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
