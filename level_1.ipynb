{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Track\n",
    "\n",
    "Welcome to the data science track. We'll be exploring some weather data to see if we can predict whether it'll rain. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 1: Scraping Data\n",
    "\n",
    "In this level, we'll obtain the data we need for future analysis. Data collection is often overlooked, but it's crucial to obtain high quality data without errors.\n",
    "\n",
    "Data comes in many different forms and from many different sources. For example, you might imagine collecting the most recent posts someone has made on Facebook. Each post would be a *data point* with multiple different *variables* (e.g. the time of the post, or the number of words in the post).\n",
    "\n",
    "For this project, we'll be analyzing data from the [Weather underground](http://www.wunderground.com/), a website for weather information.\n",
    "\n",
    "So, how should we get our data? Well, one way would be to visit the website and copy-paste things into a spreadsheet. But, as you'd imagine, that gets very tedious very quickly. Why not make the computer do it for us?\n",
    "\n",
    "Using a computer to automatically collect data from websites is called **web scraping**. Certain websites have things called **APIs** which are essentially pages specifically for computers extracting information, but there's still a lot of data locked-up without any APIs.\n",
    "\n",
    "The Weather Underground actually has an API [available](http://www.wunderground.com/weather/api/), but we'll do it the hard way to show you how web scraping would work in the first place.\n",
    "\n",
    "To start collecting our data, let's look at a [sample webpage](http://www.wunderground.com/history/airport/KNYC/2016/1/1/DailyHistory.html) that we may extract information from:\n",
    "\n",
    "![Wunderground Screenshot](wunderground.png)\n",
    "\n",
    "The page organizes information into a nice, neat table, which is good for us. Let's dig into this a little more by exploring how the webpage is presenting this information.\n",
    "\n",
    "![Inspect Screenshot](inspect.png)\n",
    "\n",
    "Let's right click on the 38$^\\circ$, and then press \"Inspect\". This will allow us to look at the actual HTML code that generated the webpage we're looking at. We can then right click where it says `tbody` and press \"Edit as HTML\".\n",
    "\n",
    "![Edit as HTML Screenshot](edit-as-html.png)\n",
    "\n",
    "This should show you the code of this table, which should look like:\n",
    "\n",
    "```html\n",
    "<tbody>\n",
    "\t\t<tr>\n",
    "\t\t<td class=\"history-table-grey-header\">Temperature</td>\n",
    "\t\t<td colspan=\"3\" class=\"history-table-grey-header\">&nbsp;</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t<td class=\"indent\"><span>Mean Temperature</span></td>\n",
    "\t\t<td>\n",
    "  <span class=\"wx-data\"><span class=\"wx-value\">38</span><span class=\"wx-unit\">&nbsp;°F</span></span>\n",
    "</td>\n",
    "\t\t<td>\n",
    "  <span class=\"wx-data\"><span class=\"wx-value\">33</span><span class=\"wx-unit\">&nbsp;°F</span></span>\n",
    "</td>\n",
    "\t\t<td>&nbsp;</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t<td class=\"indent\"><span>Max Temperature</span></td>\n",
    "\t\t<td>\n",
    "  <span class=\"wx-data\"><span class=\"wx-value\">42</span><span class=\"wx-unit\">&nbsp;°F</span></span>\n",
    "</td>\n",
    "\t\t<td>\n",
    "  <span class=\"wx-data\"><span class=\"wx-value\">39</span><span class=\"wx-unit\">&nbsp;°F</span></span>\n",
    "</td>\n",
    "\t\t<td>\n",
    "  <span class=\"wx-data\"><span class=\"wx-value\">62</span><span class=\"wx-unit\">&nbsp;°F</span></span>\n",
    "(1966)</td>\n",
    "\t\t</tr>\n",
    "```\n",
    "\n",
    "Here, we're seeing the raw HTML that the website is amde up of. Although it might take some squinting, we can see that each `tr` (table row) represents one variable that we might be interested in.\n",
    "\n",
    "Now, how much data do we want? To conveniently side-step leap years, let's say we're interested in dat abetween January 1, 2013 and December 31, 2015.\n",
    "\n",
    "Now, let's think more about what scope of data we want; we probably want at least a couple of years' worth of data. Let's say that we're interested in data between January 1, 2013 and December 31, 2015. Another nice thing about the Weather Underground is the URL structure: the URL for the page above is \"http://www.wunderground.com/history/airport/KNYC/2016/1/1/DailyHistory.html\" for January 1<sup>st</sup>, 2016. Luckily, it turns out that we can replace the `2016/1/1` with the year, month, and day that we're interested to obtain the right webpage. That means we can start by generating all the URLs we're interested with Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "days_per_month = {1: 31, 2: 28, 3: 31, 4: 30,\n",
    "                  5: 31, 6: 30, 7: 31, 8: 31,\n",
    "                  9: 30, 10: 31, 11: 30, 12: 31}\n",
    "\n",
    "link_format = \"http://www.wunderground.com/history/airport/KNYC/{}/{}/{}/DailyHistory.html\"\n",
    "links = [link_format.format(year, month, day)\n",
    "         for year in range(2013, 2016) # 2013 - 2015 inclusive\n",
    "         for month in range(1, 13)     # 1 - 12 inclusive\n",
    "         for day in range(1, days_per_month[month] + 1)]\n",
    "\n",
    "print(len(links))\n",
    "print(\"\\n\".join(links[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our code is working! We have exactly 3 year's worth of links (3 * 365 = 1095), which is a nice little sanity check. Now that we have all the links, we can start downloading the webpages, using a neat library called `requests`. We'll save all the webpages locally, so that we don't have to keep redownloading things if we need to redo our analysis.\n",
    "\n",
    "(Note: this will take a while, and that's okay. Shouldn't be more than 10 or so minutes! If you're interested in making this go faster, look into `requests-futures`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os.path\n",
    "\n",
    "for i, link in enumerate(links):\n",
    "    if i % 50 == 0:\n",
    "        print(\"Done with %d..\" % i)\n",
    "        \n",
    "    fname = \"{}.html\".format(i)\n",
    "    if os.path.isfile(fname):\n",
    "        continue\n",
    "    with open(fname, \"w\") as fout:\n",
    "        r = requests.get(link)\n",
    "        fout.write(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use a library called Beautiful Soup to parse and explore the HTML pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"0.html\") as fin:\n",
    "    soup = BeautifulSoup(fin.read(), \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Beautiful Soup, we can look for particular things on different pages. For example, we can look for the links (`a` tags) on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_as = soup.find_all('a')\n",
    "for i in range(5):\n",
    "    print(all_as[-i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the specific data we're looking for, it's all inside of a `table` with `historyTable` as the id. We can also search by id with BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_table = soup.find(id='historyTable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look for `tr`'s within this table specifically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows = main_table.find_all('tr')\n",
    "print(len(rows))\n",
    "for i in range(3):\n",
    "    print(rows[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have an actual row, there's lots of information we can extract. Let's try it with one of the rows.\n",
    "\n",
    "(We're only interested in the first two cells because those are the row name and value on that day.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row = rows[2]\n",
    "for cell in row.find_all('td'):\n",
    "    print(cell)\n",
    "    print()\n",
    "row_name = row.find_all('td')[0].text.strip()  # Get rid of extra whitespace\n",
    "row_value = row.find_all('td')[1].text.strip()\n",
    "print(row_name, \":\", row_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderful! Let's write some code that can do this for all of the rows in the table we had above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for row in rows:\n",
    "    #Only process the rows with 4 cells to eliminate heading rows, etc.\n",
    "    if len(row.find_all('td')) == 4:\n",
    "        row_name = row.find_all('td')[0].text.strip()\n",
    "        row_value = row.find_all('td')[1].text.strip() \n",
    "        print(row_name, \":\", row_value)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa! We now have tangible data for January 1<sup>st</sup>. To make things simpler, let's use the 'Mean Temperature', 'Max Temperature', 'Min Temperature', 'Dew Point', 'Average Humidity', 'Maximum Humidity', 'Minimum Humidity', 'Precipitation', 'Wind Speed', 'Max Wind Speed', and 'Max Gust Speed' variables (also known as fields) from here out. Let's write a function that can scrape one HTML file given its name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fields = ['Mean Temperature', 'Max Temperature', 'Min Temperature',\\\n",
    "          'Dew Point', 'Average Humidity', 'Maximum Humidity',\\\n",
    "          'Minimum Humidity', 'Precipitation', 'Wind Speed',\\\n",
    "          'Max Wind Speed', 'Max Gust Speed']\n",
    "def scrape_file(name):\n",
    "    with open(name) as fin:\n",
    "        soup = BeautifulSoup(fin.read(), \"html.parser\")\n",
    "    data = {}\n",
    "    for row in soup.find(id=\"historyTable\").find_all(\"tr\"):\n",
    "        cells = row.find_all(\"td\")\n",
    "        if len(cells) == 4:\n",
    "            name = cells[0].text.strip()\n",
    "            if name in fields:\n",
    "                data[name] = cells[1].text.split()[0].strip()   # Split to remove units\n",
    "    return data\n",
    "scrape_file(\"0.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woo, we're making progress! Now that we can extract the data we want from any general HTML, it isn't too much more work to put together all of the data. We'll be storing all of this data in a special type of file called a *Comma Separated Values* (CSV) file; this just means a file that looks something like this:\n",
    "\n",
    "```\n",
    "A,B,C\n",
    "1,2,3\n",
    "5,10,15\n",
    "```\n",
    "\n",
    "It is essentially equivalent to a spreadsheet that is like this:\n",
    "\n",
    "|  A  |  B  |  C  |\n",
    "| :-: | :-: | :-: |\n",
    "|  1  |  2  |  3  |\n",
    "|  5  | 10  | 15  |\n",
    "\n",
    "The useful thing about CSV files is that it is an extremely common data format that data science tools utilize. So common, in fact, that Python has a builtin tools to write them. Let's get started on writing out our CSV file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_fields = [\"Month\", \"Day\", \"Year\"] + fields\n",
    "\n",
    "with open(\"weather_data.csv\", \"w\") as fout:\n",
    "    writer = csv.DictWriter(fout, csv_fields)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for i, link in enumerate(links):\n",
    "        if i % 50 == 0:\n",
    "            print(\"Done with \", i)\n",
    "        \n",
    "        data = scrape_file(\"{}.html\".format(i))\n",
    "        url_parts = link.split(\"/\")\n",
    "        data[\"Month\"] = int(url_parts[-3])\n",
    "        data[\"Year\"] = int(url_parts[-4])\n",
    "        data[\"Day\"] = int(url_parts[-2])\n",
    "        \n",
    "        writer.writerow(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
