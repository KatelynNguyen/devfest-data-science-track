{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Curriculum\n",
    "\n",
    "In this curriculum, we'll be exploring how to build a data science project. Our end goal is a simple interactive graphic that is able to predict how much precipitation we will receive on a certain day given some information about the rest of the weather (temperature, humidity, etc.) on that day. Along the way, we'll see how to scrape, model, and visualize data; we'll be using Python for this project. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 1: Scraping Data\n",
    "\n",
    "In this level, we'll be obtaining the data that we hope to analyze later; this is an often overlooked part of data science, but it is crucial because errors in obtaining the data can cause problems for further analysis.\n",
    "\n",
    "Data can come in many different forms and originate from many sources; for example, you could imagine collecting the 5 most recent posts all of your friends on Facebook have made. Each of these posts would be considered as one data point; we can observe different things about these data (continuing the Facebook example, the number of words in the post, whether the post contains a photo, what time it was posted, etc.), and these are called *variables*. Often times, we want to observe something about a data point given certain information about the data point; in the Facebook example, we could try to predict whether there is a photo in the post based on other information about the post.\n",
    "\n",
    "For this project, we'll be analyzing data from [Wunderground](http://www.wunderground.com/); this is a website that contains information about the weather. What we'll want to do is assemble a dataset of weather over the past couple of years to see if we can predict the amount of precipitation on a certain day based on other information about that day. \n",
    "\n",
    "This brings us to our first important lesson: *obtaining the data must be scalable*. Let's think about how we can assemble such a dataset; one way is to navigate the site ourselves for each day we want to analyze while recording the variables we wish to analyze. Unfortunately, this won't work if we want to analyze a lot of data because it'll take a long time to collect all of the data needed. We're lucky because we can in fact automate the collection of this data, meaning it'll take much less time.\n",
    "\n",
    "The way we'll collect the data is by a process called **web scraping**. It turns out that a lot of important and valuable information is just located on webpages. We can programatically find the data we're looking for and then extract exactly the information we want.\n",
    "\n",
    "To start collecting our data, let's look at a sample webpage that we may extract information from:\n",
    "\n",
    "![Wunderground Screenshot](wunderground.png)\n",
    "\n",
    "This webpage come from the following link: \"http://www.wunderground.com/history/airport/KNYC/2016/1/1/DailyHistory.html\". It looks like this page organizes the information well, which is good for us! It clearly denotes where the temperature for the day is located. Let's dig into this a little more by exploring how the webpage is presenting this information.\n",
    "\n",
    "![Inspect Screenshot](inspect.png)\n",
    "\n",
    "Let's right click on the 38$^\\circ$, and then press \"Inspect\". This will allow us to look at the actual HTML code that generated the webpage we're looking at. We can then right click where it says tbody and press \"Edit as HTML\".\n",
    "\n",
    "![Edit as HTML Screenshot](edit-as-html.png)\n",
    "\n",
    "This should show you the code of this table, and the first lines should look like this:\n",
    "```\n",
    "<tbody>\n",
    "\t\t<tr>\n",
    "\t\t<td class=\"history-table-grey-header\">Temperature</td>\n",
    "\t\t<td colspan=\"3\" class=\"history-table-grey-header\">&nbsp;</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t<td class=\"indent\"><span>Mean Temperature</span></td>\n",
    "\t\t<td>\n",
    "  <span class=\"wx-data\"><span class=\"wx-value\">38</span><span class=\"wx-unit\">&nbsp;°F</span></span>\n",
    "</td>\n",
    "\t\t<td>\n",
    "  <span class=\"wx-data\"><span class=\"wx-value\">33</span><span class=\"wx-unit\">&nbsp;°F</span></span>\n",
    "</td>\n",
    "\t\t<td>&nbsp;</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t<td class=\"indent\"><span>Max Temperature</span></td>\n",
    "\t\t<td>\n",
    "  <span class=\"wx-data\"><span class=\"wx-value\">42</span><span class=\"wx-unit\">&nbsp;°F</span></span>\n",
    "</td>\n",
    "\t\t<td>\n",
    "  <span class=\"wx-data\"><span class=\"wx-value\">39</span><span class=\"wx-unit\">&nbsp;°F</span></span>\n",
    "</td>\n",
    "\t\t<td>\n",
    "  <span class=\"wx-data\"><span class=\"wx-value\">62</span><span class=\"wx-unit\">&nbsp;°F</span></span>\n",
    "(1966)</td>\n",
    "\t\t</tr>\n",
    "```\n",
    "\n",
    "Here, we're seeing exactly how the information is being presented to the browser. At this level, it may be a little tough to see, but essentially, each row of the table (starts with `<tr>` and ends with `</tr>` has more information about the day that we may be interested in.) This organization means that we can write a program that will extract the data we want. Awesome!\n",
    "\n",
    "Now, let's think more about what scope of data we want; we probably want at least a couple of years' worth of data. Let's say that we're interested in data between January 1, 2013 and December 31, 2015. Now, we just need to write code that will allow us to get all of the links to the data we want. If you remember from above, the link was \"http://www.wunderground.com/history/airport/KNYC/2016/1/1/DailyHistory.html\" for January 1<sup>st</sup>, 2016. Thus, it seems likely that we only need to substitute the year, month, and day that we want data for, which is convenient. Let's get into some Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_year = 2013\n",
    "end_year = 2015\n",
    "days_per_month = {1: 31, 2: 28, 3: 31, 4: 30,\n",
    "                     5: 31, 6: 30, 7: 31, 8: 31,\n",
    "                     9: 30, 10: 31, 11: 30, 12: 31}\n",
    "\n",
    "link_format = \"http://www.wunderground.com/history/airport/KNYC/{}/{}/{}/DailyHistory.html\"\n",
    "links = [link_format.format(year, month, day)\n",
    "         for year in range(start_year, end_year + 1)\n",
    "         for month in range(1, 13)     # 1 - 12 inclusive\n",
    "         for day in range(1, days_per_month[month] + 1)]\n",
    "\n",
    "print(len(links))\n",
    "print(\"\\n\".join(links[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our code is working! As a sanity check, 365 * 3 = 1095, which means we have the expected number of links. Let's start by downloading all of these webpages to our computer to make the process of extracting the information later on. We'll be using the `requests` package to download the webpage.\n",
    "\n",
    "(Note: this will take a while, and that's okay. Shouldn't be more than 10 or so minutes!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os.path\n",
    "\n",
    "for i, link in enumerate(links):\n",
    "    if i % 50 == 0:\n",
    "        print(\"Done with %d..\" % i)\n",
    "        \n",
    "    fname = \"{}.html\".format(i)\n",
    "    if os.path.isfile(fname):\n",
    "        continue\n",
    "    with open(fname, \"w\") as fout:\n",
    "        r = requests.get(link)\n",
    "        fout.write(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we have all the data stored locally on our computer! This was done so that analyzing it won't take as much time since all the data is local rather than on the webpage.\n",
    "\n",
    "Let's try exploring one of the HTML pages using Beautiful Soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"0.html\") as fin:\n",
    "    soup = BeautifulSoup(fin.read(), \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Beautiful Soup, we can look for particular things on different pages. For example, we can look for the links (`a` tags) on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_as = soup.find_all('a')\n",
    "for i in range(5):\n",
    "    print(all_as[-i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the specific data we're looking for, it's all inside of a `table` with `historyTable` as the id. We can also search by id with BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_table = soup.find(id='historyTable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look for `tr`'s within this table specifically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows = main_table.find_all('tr')\n",
    "print(len(rows))\n",
    "for i in range(3):\n",
    "    print(rows[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have an actual row, there's lots of information we can extract. Let's try it with one of the rows.\n",
    "\n",
    "(We're only interested in the first two cells because those are the row name and value on that day.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row = rows[2]\n",
    "for cell in row.find_all('td'):\n",
    "    print(cell)\n",
    "    print()\n",
    "row_name = row.find_all('td')[0].text.strip()  # Get rid of extra whitespace\n",
    "row_value = row.find_all('td')[1].text.strip()\n",
    "print(row_name, \":\", row_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderful! Let's write some code that can do this for all of the rows in the table we had above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for row in rows:\n",
    "    #Only process the rows with 4 cells to eliminate heading rows, etc.\n",
    "    if len(row.find_all('td')) == 4:\n",
    "        row_name = row.find_all('td')[0].text.strip()\n",
    "        row_value = row.find_all('td')[1].text.strip() \n",
    "        print(row_name, \":\", row_value)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa! We now have tangible data for January 1<sup>st</sup>. To make things simpler, let's use the 'Mean Temperature', 'Max Temperature', 'Min Temperature', 'Dew Point', 'Average Humidity', 'Maximum Humidity', 'Minimum Humidity', 'Precipitation', 'Wind Speed', 'Max Wind Speed', and 'Max Gust Speed' variables (also known as fields) from here out. Let's write a function that can scrape one HTML file given its name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fields = ['Mean Temperature', 'Max Temperature', 'Min Temperature',\\\n",
    "          'Dew Point', 'Average Humidity', 'Maximum Humidity',\\\n",
    "          'Minimum Humidity', 'Precipitation', 'Wind Speed',\\\n",
    "          'Max Wind Speed', 'Max Gust Speed']\n",
    "def scrape_file(name):\n",
    "    with open(name) as fin:\n",
    "        soup = BeautifulSoup(fin.read(), \"html.parser\")\n",
    "    data = {}\n",
    "    for row in soup.find(id=\"historyTable\").find_all(\"tr\"):\n",
    "        cells = row.find_all(\"td\")\n",
    "        if len(cells) == 4:\n",
    "            name = cells[0].text.strip()\n",
    "            if name in fields:\n",
    "                data[name] = cells[1].text.split()[0].strip()   # Split to remove units\n",
    "    return data\n",
    "scrape_file(\"0.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woo, we're making progress! Now that we can extract the data we want from any general HTML, it isn't too much more work to put together all of the data. We'll be storing all of this data in a special type of file called a *Comma Separated Values* (CSV) file; this just means a file that looks something like this:\n",
    "\n",
    "```\n",
    "A,B,C\n",
    "1,2,3\n",
    "5,10,15\n",
    "```\n",
    "\n",
    "It is essentially equivalent to a spreadsheet that is like this:\n",
    "\n",
    "|  A  |  B  |  C  |\n",
    "| :-: | :-: | :-: |\n",
    "|  1  |  2  |  3  |\n",
    "|  5  | 10  | 15  |\n",
    "\n",
    "The useful thing about CSV files is that it is an extremely common data format that data science tools utilize. So common, in fact, that Python has a builtin tools to write them. Let's get started on writing out our CSV file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_fields = [\"Month\", \"Day\", \"Year\"] + fields\n",
    "\n",
    "with open(\"weather_data.csv\", \"w\") as fout:\n",
    "    writer = csv.DictWriter(fout, csv_fields)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for i, link in enumerate(links):\n",
    "        if i % 50 == 0:\n",
    "            print(\"Done with \", i)\n",
    "        \n",
    "        data = scrape_file(\"{}.html\".format(i))\n",
    "        url_parts = link.split(\"/\")\n",
    "        data[\"Month\"] = int(url_parts[-3])\n",
    "        data[\"Year\"] = int(url_parts[-4])\n",
    "        data[\"Day\"] = int(url_parts[-2])\n",
    "        \n",
    "        writer.writerow(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
