{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Track\n",
    "\n",
    "Welcome to the data science track! :D <br>\n",
    "We'll be exploring some weather data to see if we can predict whether it'll rain, so let's get started!\n",
    "\n",
    "# Level 0: Environment Setup\n",
    "\n",
    "## Using Conda\n",
    "\n",
    "We highly recommend that you download and use\n",
    "[Conda](https://www.continuum.io/downloads), an open-source package\n",
    "manager for scientific computing. Why? Well, scientific computing\n",
    "packages typically link to some heavy-duty C and Fortran libraries.\n",
    "Conda makes it extremely easy to install these cross-language\n",
    "dependencies easily on all major platforms. If you don't use Conda,\n",
    "you might have to spend time messing around with installing an entire\n",
    "Fortran toolchain, which is definitely not fun (especially on Windows).\n",
    "\n",
    "The easiest way to install Conda is with Anaconda (make sure you pick\n",
    "Python 3), which is essentially Conda bundled with two dozen commonly\n",
    "installed dependencies. Anaconda comes with\n",
    "[Spyder](https://en.wikipedia.org/wiki/Spyder_%28software%29) a nice IDE\n",
    "for Python data science.  You can also use a [Jupyter\n",
    "notebook](http://jupyter.org/), which lets you code in the browser (this\n",
    "curriculum was originally developed on a Jupyter notebook, and we do\n",
    "make use of a couple Jupyter-specific commands).\n",
    "\n",
    "Be warned, though: Anaconda installs a _lot_ of packages.  If you're\n",
    "strapped for hard drive space, consider installing\n",
    "[Miniconda](http://conda.pydata.org/miniconda.html), the minimal\n",
    "distribution with Conda.\n",
    "After installing Conda, open up a command prompt (`cmd.exe` for Windows\n",
    "and `Terminal` for Mac) and type:\n",
    "\n",
    "```bash\n",
    "$ conda update conda\n",
    "$ conda install bokeh matplotlib notebook numpy pandas requests scikit-learn seaborn\n",
    "```\n",
    "\n",
    "For Conda power users, you also use the following `environment.yml` file:\n",
    "\n",
    "```\n",
    "name: devfest\n",
    "dependencies:\n",
    "- beautifulsoup4=4.4\n",
    "- matplotlib=1.5.1\n",
    "- notebook=4.1.0\n",
    "- numpy=1.10.2\n",
    "- pandas=0.17.1\n",
    "- python=3.5\n",
    "- requests=2.9.1\n",
    "- seaborn=0.7.0\n",
    "```\n",
    "\n",
    "## Without Conda\n",
    "\n",
    "If you're not using Conda, then we recommend you install Python 3 from\n",
    "the [Python website](https://www.python.org/). Certain computers may\n",
    "come with a version of Python installed: if you see something like:\n",
    "\n",
    "```bash\n",
    "$ python3\n",
    "Python 3.5.1 (default, Dec  7 2015, 11:16:01)\n",
    "[GCC 4.8.4] on linux\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    ">>>\n",
    "```\n",
    "\n",
    "when you type `python3` onto a terminal, then you're good to go (don't\n",
    "worry too much if you have Python 3.3 or Python 3.4 instead of Python\n",
    "3.5 -- our code should be compatible with all three versions).\n",
    "\n",
    "Python 3 should automatically come with `pip`, the Python package\n",
    "manager. Open up a terminal and type:\n",
    "\n",
    "```bash\n",
    "$ pip3 install BeautifulSoup4\n",
    "$ pip3 install bokeh\n",
    "$ pip3 install ipython\n",
    "$ pip3 install jupyter\n",
    "$ pip3 install matplotlib\n",
    "$ pip3 install numpy\n",
    "$ pip3 install pandas\n",
    "$ pip3 install requests\n",
    "$ pip3 install scikit-learn\n",
    "$ pip3 install scipy\n",
    "$ pip3 install seaborn\n",
    "```\n",
    "\n",
    "On Windows, you might need to visit [Christoph Gohlke's Unofficial\n",
    "Windows Binaries](http://www.lfd.uci.edu/~gohlke/pythonlibs/) to get\n",
    "things to install correctly.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "Open a terminal (or `cmd.exe`) and run:\n",
    "\n",
    "```bash\n",
    "$ jupyter notebook\n",
    "```\n",
    "\n",
    "a Jupyter notebook window should pop-up. Just create a new Python 3 notebook and you should be good to go.\n",
    "\n",
    "If you decide to use something other than a Jupyter notebook, note that we used some Jupyter-specific We wrote this curriculum using [Jupyter notebooks](http://jupyter.org/), so there may be some slight finnegaling required (such as omitting `%matplotlib inline` and `bokeh.io.output_notebook()`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 1: Scraping Data\n",
    "\n",
    "(Find this notebook hosted [here](http://nbviewer.jupyter.org/github/kl2806/devfest-data-science-track/blob/master/level_1.ipynb).)\n",
    "\n",
    "In this level, we'll obtain the data we need for future analysis. Data collection is often overlooked, but it's crucial to obtain high quality data without errors.\n",
    "\n",
    "Data comes in many different forms and from many different sources. For example, you might imagine collecting the most recent posts someone has made on Facebook. Each post would be a *data point* with multiple different *variables* (e.g. the time of the post, or the number of words in the post).\n",
    "\n",
    "For this project, we'll be analyzing data from the [Weather Underground](http://www.wunderground.com/), a website for weather information.\n",
    "\n",
    "So, how should we get our data? Well, one way would be to visit the website and copy-paste things into a spreadsheet. But, as you'd imagine, that gets very tedious very quickly. Why not make the computer do it for us?\n",
    "\n",
    "Using a computer to automatically collect data from websites is called **web scraping**. Certain websites have things called **APIs** which are essentially pages specifically for computers extracting information, but there's still a lot of data locked-up without any APIs.\n",
    "\n",
    "The Weather Underground actually has an API [available](http://www.wunderground.com/weather/api/), but we'll do it the hard way to show you how web scraping would work in the first place.\n",
    "\n",
    "To start collecting our data, let's look at a [sample webpage](http://www.wunderground.com/history/airport/KNYC/2016/1/1/DailyHistory.html) that we may extract information from:\n",
    "\n",
    "![Wunderground Screenshot](wunderground.png)\n",
    "\n",
    "The page organizes information into a nice, neat table, which is good for us. Let's dig into this a little more by exploring how the webpage is presenting this information.\n",
    "\n",
    "![Inspect Screenshot](inspect.png)\n",
    "\n",
    "Let's right click on the 38$^\\circ$, and then press \"Inspect\". This will allow us to look at the actual HTML code that generated the webpage we're looking at. We can then right click where it says `tbody` and press \"Edit as HTML\".\n",
    "\n",
    "![Edit as HTML Screenshot](edit-as-html.png)\n",
    "\n",
    "This should show you the code of this table, which should look like:\n",
    "\n",
    "```html\n",
    "<tbody>\n",
    "\t\t<tr>\n",
    "\t\t<td class=\"history-table-grey-header\">Temperature</td>\n",
    "\t\t<td colspan=\"3\" class=\"history-table-grey-header\">&nbsp;</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t<td class=\"indent\"><span>Mean Temperature</span></td>\n",
    "\t\t<td>\n",
    "  <span class=\"wx-data\"><span class=\"wx-value\">38</span><span class=\"wx-unit\">&nbsp;°F</span></span>\n",
    "</td>\n",
    "\t\t<td>\n",
    "  <span class=\"wx-data\"><span class=\"wx-value\">33</span><span class=\"wx-unit\">&nbsp;°F</span></span>\n",
    "</td>\n",
    "\t\t<td>&nbsp;</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t<td class=\"indent\"><span>Max Temperature</span></td>\n",
    "\t\t<td>\n",
    "  <span class=\"wx-data\"><span class=\"wx-value\">42</span><span class=\"wx-unit\">&nbsp;°F</span></span>\n",
    "</td>\n",
    "\t\t<td>\n",
    "  <span class=\"wx-data\"><span class=\"wx-value\">39</span><span class=\"wx-unit\">&nbsp;°F</span></span>\n",
    "</td>\n",
    "\t\t<td>\n",
    "  <span class=\"wx-data\"><span class=\"wx-value\">62</span><span class=\"wx-unit\">&nbsp;°F</span></span>\n",
    "(1966)</td>\n",
    "\t\t</tr>\n",
    "```\n",
    "\n",
    "Here, we're seeing the raw HTML that the website is amde up of. Although it might take some squinting, we can see that each `tr` (table row) represents one variable that we might be interested in.\n",
    "\n",
    "Now, how much data do we want? To conveniently side-step leap years, let's say we're interested in data between January 1, 2013 and December 31, 2015.\n",
    "\n",
    "Now, let's think more about what scope of data we want; we probably want at least a couple of years' worth of data. Let's say that we're interested in data between January 1, 2013 and December 31, 2015. Another nice thing about the Weather Underground is the URL structure: the URL for the page above is \"http://www.wunderground.com/history/airport/KNYC/2016/1/1/DailyHistory.html\" for January 1<sup>st</sup>, 2016. Luckily, it turns out that we can replace the `2016/1/1` with the year, month, and day that we're interested to obtain the right webpage. That means we can start by generating all the URLs we're interested with Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1095\n",
      "http://www.wunderground.com/history/airport/KNYC/2013/1/1/DailyHistory.html\n",
      "http://www.wunderground.com/history/airport/KNYC/2013/1/2/DailyHistory.html\n",
      "http://www.wunderground.com/history/airport/KNYC/2013/1/3/DailyHistory.html\n",
      "http://www.wunderground.com/history/airport/KNYC/2013/1/4/DailyHistory.html\n",
      "http://www.wunderground.com/history/airport/KNYC/2013/1/5/DailyHistory.html\n"
     ]
    }
   ],
   "source": [
    "days_per_month = {1: 31, 2: 28, 3: 31, 4: 30,\n",
    "                  5: 31, 6: 30, 7: 31, 8: 31,\n",
    "                  9: 30, 10: 31, 11: 30, 12: 31}\n",
    "\n",
    "link_format = \"http://www.wunderground.com/history/airport/KNYC/{}/{}/{}/DailyHistory.html\"\n",
    "links = [link_format.format(year, month, day)\n",
    "         for year in range(2013, 2016) # 2013 - 2015 inclusive\n",
    "         for month in range(1, 13)     # 1 - 12 inclusive\n",
    "         for day in range(1, days_per_month[month] + 1)]\n",
    "\n",
    "print(len(links))\n",
    "print(\"\\n\".join(links[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our code is working! We have exactly 3 year's worth of links (3 * 365 = 1095), which is a nice little sanity check. Now that we have all the links, we can start downloading the webpages using a neat library called `requests`.\n",
    "\n",
    "We'll also save all the webpages locally, so that we don't have to keep re-downloading things if we need to redo our analysis. (Don't worry if the next block of code takes a little bit to run, it is downloading over 1000 files after all!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 0..\n",
      "Done with 50..\n",
      "Done with 100..\n",
      "Done with 150..\n",
      "Done with 200..\n",
      "Done with 250..\n",
      "Done with 300..\n",
      "Done with 350..\n",
      "Done with 400..\n",
      "Done with 450..\n",
      "Done with 500..\n",
      "Done with 550..\n",
      "Done with 600..\n",
      "Done with 650..\n",
      "Done with 700..\n",
      "Done with 750..\n",
      "Done with 800..\n",
      "Done with 850..\n",
      "Done with 900..\n",
      "Done with 950..\n",
      "Done with 1000..\n",
      "Done with 1050..\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os.path\n",
    "\n",
    "def download_file(link, name):\n",
    "    if os.path.isfile(name):\n",
    "        return\n",
    "    file = open(name, 'w')\n",
    "    r = requests.get(link)\n",
    "    file.write(r.text)\n",
    "    file.close()\n",
    "for i, link in enumerate(links):\n",
    "    if i % 50 == 0:\n",
    "        print(\"Done with %d..\" % i)\n",
    "    download_file(link, \"%d.html\" % i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use a library called Beautiful Soup to parse and explore the HTML pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"0.html\") as fin:\n",
    "    soup = BeautifulSoup(fin.read(), \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Beautiful Soup, we can look for particular things on different pages. For example, we can look for the links (`a` tags) on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"https://www.wunderground.com/member/registration\">\n",
      "<i class=\"fi-torso sidebar-icon\"></i> Sign Up / Sign In\n",
      "  </a>\n",
      "\n",
      "<a href=\"/wutv/?cm_ven=wutv_toast\">\n",
      "<iframe class=\"underlay\" frameborder=\"no\" id=\"ustream-tdu-player\" src=\"//www.ustream.tv/embed/21416049\" width=\"240\"></iframe>\n",
      "</a>\n",
      "\n",
      "<a class=\"modal-close close\">×</a>\n",
      "\n",
      "<a aria-label=\"Close\" class=\"close-reveal-modal\">×</a>\n",
      "\n",
      "<a class=\"button medium radius\" href=\"/member/registration\">Remove Ads</a>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_as = soup.find_all('a')\n",
    "for i in range(5):\n",
    "    print(all_as[-i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the specific data we're looking for, it's all inside of a `table` with `historyTable` as the id. We can also search by id with BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_table = soup.find(id='historyTable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look for `tr`'s within this table specifically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "<tr>\n",
      "<th> </th>\n",
      "<th>Actual</th>\n",
      "<th>Average </th>\n",
      "<th>Record </th>\n",
      "</tr>\n",
      "\n",
      "<tr>\n",
      "<td class=\"history-table-grey-header\">Temperature</td>\n",
      "<td class=\"history-table-grey-header\" colspan=\"3\"> </td>\n",
      "</tr>\n",
      "\n",
      "<tr>\n",
      "<td class=\"indent\"><span>Mean Temperature</span></td>\n",
      "<td>\n",
      "<span class=\"wx-data\"><span class=\"wx-value\">33</span><span class=\"wx-unit\"> °F</span></span>\n",
      "</td>\n",
      "<td>\n",
      "<span class=\"wx-data\"><span class=\"wx-value\">33</span><span class=\"wx-unit\"> °F</span></span>\n",
      "</td>\n",
      "<td> </td>\n",
      "</tr>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows = main_table.find_all('tr')\n",
    "print(len(rows))\n",
    "for i in range(3):\n",
    "    print(rows[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have an actual row, there's lots of information we can extract. Let's try it with one of the rows.\n",
    "\n",
    "(We're only interested in the first two cells because those are the row name and value on that day.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<td class=\"indent\"><span>Mean Temperature</span></td>\n",
      "\n",
      "<td>\n",
      "<span class=\"wx-data\"><span class=\"wx-value\">33</span><span class=\"wx-unit\"> °F</span></span>\n",
      "</td>\n",
      "\n",
      "<td>\n",
      "<span class=\"wx-data\"><span class=\"wx-value\">33</span><span class=\"wx-unit\"> °F</span></span>\n",
      "</td>\n",
      "\n",
      "<td> </td>\n",
      "\n",
      "Mean Temperature : 33 °F\n"
     ]
    }
   ],
   "source": [
    "row = rows[2]\n",
    "for cell in row.find_all('td'):\n",
    "    print(cell)\n",
    "    print()\n",
    "row_name = row.find_all('td')[0].text.strip()  # Get rid of extra whitespace\n",
    "row_value = row.find_all('td')[1].text.strip()\n",
    "print(row_name, \":\", row_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderful! Let's write some code that can do this for all of the rows in the table we had above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Temperature : 33 °F\n",
      "Max Temperature : 40 °F\n",
      "Min Temperature : 26 °F\n",
      "Heating Degree Days : 32\n",
      "Month to date heating degree days : 32\n",
      "Since 1 July heating degree days : 1622\n",
      "Cooling Degree Days : 0\n",
      "Month to date cooling degree days : 0\n",
      "Year to date cooling degree days : 0\n",
      "Dew Point : 22 °F\n",
      "Average Humidity : 54\n",
      "Maximum Humidity : 64\n",
      "Minimum Humidity : 44\n",
      "Precipitation : 0.00 in\n",
      "Month to date precipitation : 0.00\n",
      "Year to date precipitation : 0.00\n",
      "Snow : 0.00 in\n",
      "Month to date snowfall : 0.0\n",
      "Since 1 July snowfall : 5.1\n",
      "Snow Depth : 0.00 in\n",
      "Sea Level Pressure : 29.90 in\n",
      "Wind Speed : 7 mph\n",
      " (WNW)\n",
      "Max Wind Speed : 15 mph\n",
      "Max Gust Speed : 26 mph\n",
      "Visibility : 10 miles\n",
      "Events : \n"
     ]
    }
   ],
   "source": [
    "for row in rows:\n",
    "    #Only process the rows with 4 cells to eliminate heading rows, etc.\n",
    "    if len(row.find_all('td')) == 4:\n",
    "        row_name = row.find_all('td')[0].text.strip()\n",
    "        row_value = row.find_all('td')[1].text.strip() \n",
    "        print(row_name, \":\", row_value)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa! We now have tangible data for January 1<sup>st</sup>. To make things simpler, let's use the 'Mean Temperature', 'Max Temperature', 'Min Temperature', 'Dew Point', 'Average Humidity', 'Maximum Humidity', 'Minimum Humidity', 'Precipitation', 'Wind Speed', 'Max Wind Speed', and 'Max Gust Speed' variables (also known as fields) from here out. Let's write a function that can scrape one HTML file given its name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Average Humidity': '54',\n",
       " 'Dew Point': '22',\n",
       " 'Max Gust Speed': '26',\n",
       " 'Max Temperature': '40',\n",
       " 'Max Wind Speed': '15',\n",
       " 'Maximum Humidity': '64',\n",
       " 'Mean Temperature': '33',\n",
       " 'Min Temperature': '26',\n",
       " 'Minimum Humidity': '44',\n",
       " 'Precipitation': '0.00',\n",
       " 'Wind Speed': '7'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields = ['Mean Temperature', 'Max Temperature', 'Min Temperature',\\\n",
    "          'Dew Point', 'Average Humidity', 'Maximum Humidity',\\\n",
    "          'Minimum Humidity', 'Precipitation', 'Wind Speed',\\\n",
    "          'Max Wind Speed', 'Max Gust Speed']\n",
    "def scrape_file(name):\n",
    "    with open(name) as fin:\n",
    "        soup = BeautifulSoup(fin.read(), \"html.parser\")\n",
    "    data = {}\n",
    "    for row in soup.find(id=\"historyTable\").find_all(\"tr\"):\n",
    "        cells = row.find_all(\"td\")\n",
    "        if len(cells) == 4:\n",
    "            name = cells[0].text.strip()\n",
    "            if name in fields:\n",
    "                data[name] = cells[1].text.split()[0].strip()   # Split to remove units\n",
    "    return data\n",
    "scrape_file(\"0.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woo, we're making progress! Now that we can extract the data we want from any general HTML, it isn't too much more work to put together all of the data. We'll be storing all of this data in a special type of file called a *Comma Separated Values* (CSV) file; this just means a file that looks something like this:\n",
    "\n",
    "```\n",
    "A,B,C\n",
    "1,2,3\n",
    "5,10,15\n",
    "```\n",
    "\n",
    "It is essentially equivalent to a spreadsheet that is like this:\n",
    "\n",
    "|  A  |  B  |  C  |\n",
    "| :-: | :-: | :-: |\n",
    "|  1  |  2  |  3  |\n",
    "|  5  | 10  | 15  |\n",
    "\n",
    "The useful thing about CSV files is that it is an extremely common data format that data science tools utilize. So common, in fact, that Python has a builtin tools to write them. Let's get started on writing out our CSV file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_fields = [\"Month\", \"Day\", \"Year\"] + fields\n",
    "\n",
    "with open(\"weather_data.csv\", \"w\") as fout:\n",
    "    writer = csv.DictWriter(fout, csv_fields)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for i, link in enumerate(links):\n",
    "        data = scrape_file(\"{}.html\".format(i))\n",
    "        url_parts = link.split(\"/\")\n",
    "        data[\"Month\"] = int(url_parts[-3])\n",
    "        data[\"Year\"] = int(url_parts[-4])\n",
    "        data[\"Day\"] = int(url_parts[-2])\n",
    "        \n",
    "        writer.writerow(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
