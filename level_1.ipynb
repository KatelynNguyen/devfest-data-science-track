{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Curriculum\n",
    "\n",
    "In this curriculum, we'll be exploring how to build a data science project. Our end goal is a simple interactive graphic that is able to predict how much precipitation we will receive on a certain day given some information about the rest of the weather (temperature, humidity, etc.) on that day. Along the way, we'll see how to scrape, model, and visualize data; we'll be using Python for this project. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 0: Environment Setup\n",
    "The first tool we'll need is [pip](https://pip.pypa.io/en/stable/installing/). Once we have that, the following lines will set up the necessary modules:\n",
    "```\n",
    "pip install pandas\n",
    "pip install bokeh\n",
    "pip install ipython\n",
    "pip install jupyter\n",
    "pip install scikit-learn\n",
    "```\n",
    "\n",
    "We wrote this curriculum using [Jupyter notebooks](http://jupyter.org/), so there may be some slight finnegaling required (such as omitting `%matplotlib inline` and `bokeh.io.output_notebook()`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 1: Scraping Data\n",
    "\n",
    "In this level, we'll be obtaining the data that we hope to analyze later; this is an often overlooked part of data science, but it is crucial because errors in obtaining the data can cause problems for further analysis.\n",
    "\n",
    "Data can come in many different forms and originate from many sources; for example, you could imagine collecting the 5 most recent posts all of your friends on Facebook have made. Each of these posts would be considered as one data point; we can observe different things about these data (continuing the Facebook example, the number of words in the post, whether the post contains a photo, what time it was posted, etc.), and these are called *variables*. Often times, we want to observe something about a data point given certain information about the data point; in the Facebook example, we could try to predict whether there is a photo in the post based on other information about the post.\n",
    "\n",
    "For this project, we'll be analyzing data from [Wunderground](http://www.wunderground.com/); this is a website that contains information about the weather. What we'll want to do is assemble a dataset of weather over the past couple of years to see if we can predict the amount of precipitation on a certain day based on other information about that day. \n",
    "\n",
    "This brings us to our first important lesson: *obtaining the data must be scalable*. Let's think about how we can assemble such a dataset; one way is to navigate the site ourselves for each day we want to analyze while recording the variables we wish to analyze. Unfortunately, this won't work if we want to analyze a lot of data because it'll take a long time to collect all of the data needed. We're lucky because we can in fact automate the collection of this data, meaning it'll take much less time.\n",
    "\n",
    "The way we'll collect the data is by a process called **web scraping**. It turns out that a lot of important and valuable information is just located on webpages. We can programatically find the data we're looking for and then extract exactly the information we want.\n",
    "\n",
    "To start collecting our data, let's look at a sample webpage that we may extract information from:\n",
    "\n",
    "![Wunderground Screenshot](wunderground.png)\n",
    "\n",
    "This webpage come from the following link: \"http://www.wunderground.com/history/airport/KNYC/2016/1/1/DailyHistory.html\". It looks like this page organizes the information well, which is good for us! It clearly denotes where the temperature for the day is located. Let's dig into this a little more by exploring how the webpage is presenting this information.\n",
    "\n",
    "![Inspect Screenshot](inspect.png)\n",
    "\n",
    "Let's right click on the 38$^\\circ$, and then press \"Inspect\". This will allow us to look at the actual HTML code that generated the webpage we're looking at. We can then right click where it says tbody and press \"Edit as HTML\".\n",
    "\n",
    "![Edit as HTML Screenshot](edit-as-html.png)\n",
    "\n",
    "This should show you the code of this table, and the first lines should look like this:\n",
    "```\n",
    "<tbody>\n",
    "\t\t<tr>\n",
    "\t\t<td class=\"history-table-grey-header\">Temperature</td>\n",
    "\t\t<td colspan=\"3\" class=\"history-table-grey-header\">&nbsp;</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t<td class=\"indent\"><span>Mean Temperature</span></td>\n",
    "\t\t<td>\n",
    "  <span class=\"wx-data\"><span class=\"wx-value\">38</span><span class=\"wx-unit\">&nbsp;°F</span></span>\n",
    "</td>\n",
    "\t\t<td>\n",
    "  <span class=\"wx-data\"><span class=\"wx-value\">33</span><span class=\"wx-unit\">&nbsp;°F</span></span>\n",
    "</td>\n",
    "\t\t<td>&nbsp;</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t<td class=\"indent\"><span>Max Temperature</span></td>\n",
    "\t\t<td>\n",
    "  <span class=\"wx-data\"><span class=\"wx-value\">42</span><span class=\"wx-unit\">&nbsp;°F</span></span>\n",
    "</td>\n",
    "\t\t<td>\n",
    "  <span class=\"wx-data\"><span class=\"wx-value\">39</span><span class=\"wx-unit\">&nbsp;°F</span></span>\n",
    "</td>\n",
    "\t\t<td>\n",
    "  <span class=\"wx-data\"><span class=\"wx-value\">62</span><span class=\"wx-unit\">&nbsp;°F</span></span>\n",
    "(1966)</td>\n",
    "\t\t</tr>\n",
    "```\n",
    "\n",
    "Here, we're seeing exactly how the information is being presented to the browser. At this level, it may be a little tough to see, but essentially, each row of the table (starts with `<tr>` and ends with `</tr>` has more information about the day that we may be interested in.) This organization means that we can write a program that will extract the data we want. Awesome!\n",
    "\n",
    "Now, let's think more about what scope of data we want; we probably want at least a couple of years' worth of data. Let's say that we're interested in data between January 1, 2013 and December 31, 2015. Now, we just need to write code that will allow us to get all of the links to the data we want. If you remember from above, the link was \"http://www.wunderground.com/history/airport/KNYC/2016/1/1/DailyHistory.html\" for January 1<sup>st</sup>, 2016. Thus, it seems likely that we only need to substitute the year, month, and day that we want data for, which is convenient. Let's get into some Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 2013..\n",
      "Done with 2014..\n",
      "Done with 2015..\n",
      "1095\n",
      "http://www.wunderground.com/history/airport/KNYC/2013/1/1/DailyHistory.html\n",
      "http://www.wunderground.com/history/airport/KNYC/2013/1/2/DailyHistory.html\n",
      "http://www.wunderground.com/history/airport/KNYC/2013/1/3/DailyHistory.html\n",
      "http://www.wunderground.com/history/airport/KNYC/2013/1/4/DailyHistory.html\n",
      "http://www.wunderground.com/history/airport/KNYC/2013/1/5/DailyHistory.html\n"
     ]
    }
   ],
   "source": [
    "list_of_links = []\n",
    "start_year = 2013\n",
    "end_year = 2015\n",
    "month_to_num_days = {1  : 31, 2  : 28, 3  : 31, 4  : 30,\n",
    "                     5  : 31, 6  : 30, 7  : 31, 8  : 31,\n",
    "                     9  : 30, 10 : 31, 11 : 30, 12 : 31}\n",
    "link_format = \"http://www.wunderground.com/history/airport/KNYC/%d/%d/%d/DailyHistory.html\"\n",
    "for year in range(start_year, end_year + 1):\n",
    "    for month in range(1, 12 + 1):\n",
    "        num_days = month_to_num_days[month]\n",
    "        for day in range(1, num_days + 1):\n",
    "            list_of_links.append(link_format % (year, month, day))\n",
    "    print(\"Done with %s..\" % year)\n",
    "print(len(list_of_links))\n",
    "for i in range(5):\n",
    "    print(list_of_links[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our code is working! As a sanity check, 365 * 3 = 1095, which means we have the expected number of links. Let's start by downloading all of these webpages to our computer to make the process of extracting the information later on. We'll be using the `requests` package to download the webpage.\n",
    "\n",
    "(Note: this will take a while, and that's okay. Shouldn't be more than 10 or so minutes!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 0..\n",
      "Done with 50..\n",
      "Done with 100..\n",
      "Done with 150..\n",
      "Done with 200..\n",
      "Done with 250..\n",
      "Done with 300..\n",
      "Done with 350..\n",
      "Done with 400..\n",
      "Done with 450..\n",
      "Done with 500..\n",
      "Done with 550..\n",
      "Done with 600..\n",
      "Done with 650..\n",
      "Done with 700..\n",
      "Done with 750..\n",
      "Done with 800..\n",
      "Done with 850..\n",
      "Done with 900..\n",
      "Done with 950..\n",
      "Done with 1000..\n",
      "Done with 1050..\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os.path\n",
    "\n",
    "def download_file(link, name):\n",
    "    if os.path.isfile(name):\n",
    "        return\n",
    "    file = open(name, 'w')\n",
    "    r = requests.get(link)\n",
    "    file.write(r.text)\n",
    "    file.close()\n",
    "for i, link in enumerate(list_of_links):\n",
    "    if i % 50 == 0:\n",
    "        print(\"Done with %d..\" % i)\n",
    "    download_file(link, \"%d.html\" % i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we have all the data stored locally on our computer! This was done so that analyzing it won't take as much time since all the data is local rather than on the webpage.\n",
    "\n",
    "Let's try exploring one of the HTML pages using Beautiful Soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "first = open('0.html', 'r')\n",
    "soup = BeautifulSoup(first.read(), \"html.parser\")\n",
    "first.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Beautiful Soup, we can look for particular things on different pages. For example, we can look for the links (`a` tags) on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<a href=\"https://www.wunderground.com/member/registration\">\n",
      "<i class=\"fi-torso sidebar-icon\"></i> Sign Up / Sign In\n",
      "  </a>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<a href=\"/wutv/?cm_ven=wutv_toast\">\n",
      "<iframe class=\"underlay\" frameborder=\"no\" id=\"ustream-tdu-player\" src=\"http://www.ustream.tv/embed/21416049\" width=\"240\"></iframe>\n",
      "</a>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<a class=\"modal-close close\">×</a>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<a aria-label=\"Close\" class=\"close-reveal-modal\">×</a>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<a class=\"button medium radius\" href=\"/member/registration\">Remove Ads</a>\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "all_as = soup.find_all('a')\n",
    "print(len(all_trs))\n",
    "for i in range(5):\n",
    "    print('-' * 100)\n",
    "    print(all_as[-i])\n",
    "    print('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the specific data we're looking for, it's all inside of a `table` with `historyTable` as the id. We can also search by id with BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_table = soup.find(id = 'historyTable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look for `tr`'s within this table specifically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<tr>\n",
      "<th> </th>\n",
      "<th>Actual</th>\n",
      "<th>Average </th>\n",
      "<th>Record </th>\n",
      "</tr>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<tr>\n",
      "<td class=\"history-table-grey-header\">Temperature</td>\n",
      "<td class=\"history-table-grey-header\" colspan=\"3\"> </td>\n",
      "</tr>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<tr>\n",
      "<td class=\"indent\"><span>Mean Temperature</span></td>\n",
      "<td>\n",
      "<span class=\"wx-data\"><span class=\"wx-value\">33</span><span class=\"wx-unit\"> °F</span></span>\n",
      "</td>\n",
      "<td>\n",
      "<span class=\"wx-data\"><span class=\"wx-value\">33</span><span class=\"wx-unit\"> °F</span></span>\n",
      "</td>\n",
      "<td> </td>\n",
      "</tr>\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "rows = main_table.find_all('tr')\n",
    "print(len(rows))\n",
    "for i in range(3):\n",
    "    print('-' * 100)\n",
    "    print(rows[i])\n",
    "    print('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have an actual row, there's lots of information we can extract. Let's try it with one of the rows.\n",
    "\n",
    "(We're only interested in the first two cells because those are the row name and value on that day.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "<td class=\"indent\"><span>Mean Temperature</span></td>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<td>\n",
      "<span class=\"wx-data\"><span class=\"wx-value\">33</span><span class=\"wx-unit\"> °F</span></span>\n",
      "</td>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<td>\n",
      "<span class=\"wx-data\"><span class=\"wx-value\">33</span><span class=\"wx-unit\"> °F</span></span>\n",
      "</td>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<td> </td>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Mean Temperature: 33 °F\n"
     ]
    }
   ],
   "source": [
    "row = rows[2]\n",
    "for cell in row.find_all('td'):\n",
    "    print('-' * 100)\n",
    "    print(cell)\n",
    "    print('-' * 100)\n",
    "row_name = row.find_all('td')[0].text.strip() #Get rid of extra whitespace\n",
    "row_value = row.find_all('td')[1].text.strip() #Same idea here\n",
    "print(\"%s: %s\" % (row_name, row_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderful! Let's write some code that can do this for all of the rows in the table we had above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Temperature: 33 °F\n",
      "Max Temperature: 40 °F\n",
      "Min Temperature: 26 °F\n",
      "Heating Degree Days: 32\n",
      "Month to date heating degree days: 32\n",
      "Since 1 July heating degree days: 1622\n",
      "Cooling Degree Days: 0\n",
      "Month to date cooling degree days: 0\n",
      "Year to date cooling degree days: 0\n",
      "Dew Point: 22 °F\n",
      "Average Humidity: 54\n",
      "Maximum Humidity: 64\n",
      "Minimum Humidity: 44\n",
      "Precipitation: 0.00 in\n",
      "Month to date precipitation: 0.00\n",
      "Year to date precipitation: 0.00\n",
      "Snow: 0.00 in\n",
      "Month to date snowfall: 0.0\n",
      "Since 1 July snowfall: 5.1\n",
      "Snow Depth: 0.00 in\n",
      "Sea Level Pressure: 29.90 in\n",
      "Wind Speed: 7 mph\n",
      " (WNW)\n",
      "Max Wind Speed: 15 mph\n",
      "Max Gust Speed: 26 mph\n",
      "Visibility: 10 miles\n",
      "Events: \n"
     ]
    }
   ],
   "source": [
    "def process_row(row):\n",
    "    if len(row.find_all('td')) == 4: #Only process the rows with 4 cells \n",
    "                                #Eliminates heading rows, etc.\n",
    "        row_name = row.find_all('td')[0].text.strip()\n",
    "        row_value = row.find_all('td')[1].text.strip() \n",
    "        print(\"%s: %s\" % (row_name, row_value))\n",
    "    \n",
    "for row in rows:\n",
    "    process_row(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa! We now have tangible data for January 1<sup>st</sup>. To make things simpler, let's use the 'Mean Temperature', 'Max Temperature', 'Min Temperature', 'Dew Point', 'Average Humidity', 'Maximum Humidity', 'Minimum Humidity', 'Precipitation', 'Wind Speed', 'Max Wind Speed', and 'Max Gust Speed' variables (also known as fields) from here out. Let's write a function that can scrape one HTML file given its name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Average Humidity': '54',\n",
       " 'Dew Point': '22',\n",
       " 'Max Gust Speed': '26',\n",
       " 'Max Temperature': '40',\n",
       " 'Max Wind Speed': '15',\n",
       " 'Maximum Humidity': '64',\n",
       " 'Mean Temperature': '33',\n",
       " 'Min Temperature': '26',\n",
       " 'Minimum Humidity': '44',\n",
       " 'Precipitation': '0.00',\n",
       " 'Wind Speed': '7'}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields = ['Mean Temperature', 'Max Temperature', 'Min Temperature',\\\n",
    "          'Dew Point', 'Average Humidity', 'Maximum Humidity',\\\n",
    "          'Minimum Humidity', 'Precipitation', 'Wind Speed',\\\n",
    "          'Max Wind Speed', 'Max Gust Speed']\n",
    "\n",
    "def scrape_file(name):\n",
    "    f = open(name, 'r')\n",
    "    soup = BeautifulSoup(f.read(), \"html.parser\")\n",
    "    f.close()\n",
    "    data_rows = soup.find(id = \"historyTable\").find_all('tr')\n",
    "    data = {}\n",
    "    for data_row in data_rows:\n",
    "        cells = data_row.find_all('td')\n",
    "        if len(cells) == 4:\n",
    "            row_name = cells[0].text.strip()\n",
    "            if row_name in fields:\n",
    "                row_value = cells[1].text.split()[0].strip() #Get rid of units\n",
    "                data[row_name] = row_value\n",
    "    return data\n",
    "scrape_file(\"0.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woo, we're making progress! Now that we can extract the data we want from any general HTML, it isn't too much more work to put together all of the data. We'll be storing all of this data in a special type of file called a *Comma Separated Values* (CSV) file; this just means a file that looks something like this:\n",
    "\n",
    "```\n",
    "A,B,C\n",
    "1,2,3\n",
    "5,10,15\n",
    "```\n",
    "\n",
    "It is essentially equivalent to a spreadsheet that is like this:\n",
    "\n",
    "|  A  |  B  |  C  |\n",
    "| :-: | :-: | :-: |\n",
    "|  1  |  2  |  3  |\n",
    "|  5  | 10  | 15  |\n",
    "\n",
    "The useful thing about CSV files is that it is an extremely common data format that data science tools utilize. Let's get started on writing out our CSV file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_file = open('weather_data.csv', 'w')\n",
    "csv_file.write('Month,Day,Year,') #We should also keep track which day each row came from\n",
    "csv_file.write(','.join(fields))\n",
    "csv_file.write('\\n')\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code writes out the headers for the CSV file; now, we have to write out the data for each day we scraped. If we use our `scrape_file` function, it won't be too difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 0..\n",
      "Done with 50..\n",
      "Done with 100..\n",
      "Done with 150..\n",
      "Done with 200..\n",
      "Done with 250..\n",
      "Done with 300..\n",
      "Done with 350..\n",
      "Done with 400..\n",
      "Done with 450..\n",
      "Done with 500..\n",
      "Done with 550..\n",
      "Done with 600..\n",
      "Done with 650..\n",
      "Done with 700..\n",
      "Done with 750..\n",
      "Done with 800..\n",
      "Done with 850..\n",
      "Done with 900..\n",
      "Done with 950..\n",
      "Done with 1000..\n",
      "Done with 1050..\n"
     ]
    }
   ],
   "source": [
    "def link_to_date(l):\n",
    "    parts = l.split('/')\n",
    "    return (parts[-3], parts[-2], parts[-4]) #(Month, Day, Year)\n",
    "\n",
    "csv_file = open('weather_data.csv', 'a')\n",
    "for i, link in enumerate(list_of_links):\n",
    "    if i % 50 == 0:\n",
    "        print('Done with %d..' % i)\n",
    "    data_for_file = scrape_file('%d.html' % i)\n",
    "    date_for_file = link_to_date(link)\n",
    "    csv_file.write('%s,%s,%s,' % (date_for_file[0], date_for_file[1], date_for_file[2]))\n",
    "    for j, field in enumerate(fields):\n",
    "        csv_file.write(data_for_file[field])\n",
    "        if j != len(fields) - 1:\n",
    "            csv_file.write(',') #Only include a comma if it's not the last field\n",
    "    csv_file.write('\\n')\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code will probably take a couple of minutes to run, but once it's done, you've successfully scraped the Wunderground website for daily weather data from 2013 to 2015! Feel free to open your CSV file in your spreadsheet program of choice and check out your awesome accomplishment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
